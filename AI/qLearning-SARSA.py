# -*- coding: utf-8 -*-
"""T3 IA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aCcWuiEK-4YyieOWyiMfxBxzfNaLnYAd

# Treinando agentes (Q-Learning e SARSA) no ambiente Taxi-v3 (Gymnasium)


Neste notebook treinaremos agentes usando **Q-Learning** e **SARSA** no ambiente **Taxi-v3** dispon√≠vel na biblioteca `gymnasium`

## üìù Detalhes do Ambiente: FrozenLake-v1 (N√£o Escorregadio)

Este experimento utilizou o ambiente **FrozenLake-v1** com o par√¢metro `is_slippery=False`, tornando-o **determin√≠stico** (a a√ß√£o escolhida sempre resulta no movimento desejado).

---

### üéØ Objetivo e Estrutura

| Detalhe | Descri√ß√£o |
| :--- | :--- |
| **Objetivo** | O agente deve navegar do **In√≠cio (S)** at√© o **Objetivo (G)**, atravessando o lago congelado e **evitando cair nos Buracos (H)**. |
| **Grade** | O ambiente √© uma grade de **$4 \times 4$**, totalizando 16 c√©lulas/estados. |

| 0 (S) | 1 (F) | 2 (F) | 3 (F) |
| :--- | :--- | :--- | :--- |
| 4 (F) | 5 (H) | 6 (F) | 7 (H) |
| 8 (F) | 9 (F) | 10 (F)| 11 (H)|
| 12 (H)| 13 (F)| 14 (F)| 15 (G)|

| S = Start | F = Frozen (Seguro) | H = Hole (Buraco/Falha) | G = Goal (Objetivo/Sucesso) |

---

### üó∫Ô∏è Espa√ßo de A√ß√µes e Observa√ß√µes

| Espa√ßo | Detalhe | Tipo | Valores |
| :--- | :--- | :--- | :--- |
| **A√ß√µes** | Movimentos que o agente pode executar. | Discreto (4) | **0: Esquerda** ($\leftarrow$), **1: Baixo** ($\downarrow$), **2: Direita** ($\rightarrow$), **3: Cima** ($\uparrow$). |
| **Observa√ß√µes** | O estado atual do agente (c√©lula da grade). | Discreto (16) | Estados de **0 a 15**. |

---

### üí∞ Recompensas

A recompensa √© esparsa (maioria dos passos retorna 0.0):

| Evento | Recompensa (`r`) | Resultado |
| :--- | :--- | :--- |
| Chegar ao **Objetivo (G)** | **$+1.0$** | Epis√≥dio termina (Sucesso) |
| Cair em um **Buraco (H)** | **$0.0$** | Epis√≥dio termina (Falha) |
| Qualquer outro passo (F) | **$0.0$** | Continua o epis√≥dio |

---

### üìà Forma√ß√£o da Tabela Q (Q-Learning / SARSA)

A **Tabela Q** √© uma matriz $16 \times 4$ que armazena o **valor esperado** de se tomar cada a√ß√£o ($a$) em cada estado ($s$).

1.  **Inicializa√ß√£o:** $Q(s, a)$ √© inicializada com **zeros**.
2.  **A√ß√£o:** O agente usa a estrat√©gia **$\epsilon$-greedy**, equilibrando a **Explora√ß√£o** ($\epsilon$) com a **Explota√ß√£o** ($1 - \epsilon$).
3.  **Atualiza√ß√£o:** A Tabela Q √© ajustada a cada passo com a **Equa√ß√£o de Bellman** (a diferen√ßa est√° no termo futuro):

| Algoritmo | Termo Futuro | Tipo de Pol√≠tica |
| :--- | :--- | :--- |
| **Q-Learning** | $\gamma \mathbf{\max_{a'} Q(s', a')}$ | **Off-policy** (Estimativa da melhor a√ß√£o futura, ignorando a pol√≠tica usada para mover). |
| **SARSA** | $\gamma \mathbf{Q(s', a')}$ | **On-policy** (Estimativa baseada na pr√≥xima a√ß√£o $a'$ que *ser√° tomada* pelo agente, seguindo a pol√≠tica $\epsilon$-greedy). |
4.  **Converg√™ncia:** O valor de $\epsilon$ **deca√≠a exponencialmente** de $1.0$ para $0.05$ ao longo dos epis√≥dios, garantindo que o agente transitasse de um comportamento puramente aleat√≥rio para um comportamento otimizado, explorando o conhecimento armazenado na Tabela Q.

Setup (instalar as depend√™ncias e importar libs)
"""

!pip install -q gymnasium==0.28.1
!pip install -q gymnasium[toy_text]
!pip install -q pandas numpy matplotlib seaborn

import gymnasium as gym
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import random
from IPython.display import clear_output

"""Fun√ß√µes utilit√°rias"""

def make_env():
    # FrozenLake: 4x4, 16 estados, 4 a√ß√µes (N, S, L, O)
    # is_slippery=False: A√ß√µes s√£o determin√≠sticas (sem chance de escorregar)
    return gym.make('FrozenLake-v1', is_slippery=False)


def epsilon_by_episode(initial_epsilon, min_epsilon, decay_rate, episode):
    # Decaimento exponencial de epsilon
    return max(min_epsilon, initial_epsilon * (decay_rate ** episode))

"""Implementa√ß√£o: Q-Learning (tabular)"""

def train_q_learning(env,
                     num_episodes=5000,
                     max_steps=100,
                     alpha=0.1,
                     gamma=0.99,
                     initial_epsilon=1.0,
                     min_epsilon=0.01,
                     decay_rate=0.999):

    n_states = env.observation_space.n
    n_actions = env.action_space.n

    q_table = np.zeros((n_states, n_actions))
    rewards_all_episodes = []
    epsilons = []

    for episode in range(num_episodes):
        state, _ = env.reset()
        total_reward = 0
        epsilon = epsilon_by_episode(initial_epsilon, min_epsilon, decay_rate, episode)
        epsilons.append(epsilon)

        for step in range(max_steps):
            # Escolha de a√ß√£o (epsilon-greedy)
            if random.random() < epsilon:
                action = random.randrange(n_actions)
            else:
                action = int(np.argmax(q_table[state]))

            # Executar a√ß√£o
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # Atualiza√ß√£o Q-Learning (Off-policy: usa max(Q) do pr√≥ximo estado)
            best_next = np.max(q_table[next_state])
            td_target = reward + gamma * best_next
            td_error = td_target - q_table[state, action]
            q_table[state, action] += alpha * td_error

            state = next_state
            total_reward += reward

            if done:
                break

        rewards_all_episodes.append(total_reward)

    return q_table, rewards_all_episodes, epsilons

"""## 5) Implementa√ß√£o SARSA (policy)"""

def train_sarsa(env,
                num_episodes=5000,
                max_steps=100,
                alpha=0.1,
                gamma=0.99,
                initial_epsilon=1.0,
                min_epsilon=0.01,
                decay_rate=0.999):

    n_states = env.observation_space.n
    n_actions = env.action_space.n

    q_table = np.zeros((n_states, n_actions))
    rewards_all_episodes = []
    epsilons = []

    for episode in range(num_episodes):
        state, _ = env.reset()

        epsilon = epsilon_by_episode(initial_epsilon, min_epsilon, decay_rate, episode)
        epsilons.append(epsilon)

        # Escolher a√ß√£o inicial via epsilon-greedy
        if random.random() < epsilon:
            action = random.randrange(n_actions)
        else:
            action = int(np.argmax(q_table[state]))

        total_reward = 0

        for step in range(max_steps):
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # Pr√≥xima a√ß√£o (on-policy) via epsilon-greedy
            if random.random() < epsilon:
                next_action = random.randrange(n_actions)
            else:
                next_action = int(np.argmax(q_table[next_state]))

            # Atualiza√ß√£o SARSA (On-policy: usa Q(s', a') da pr√≥xima a√ß√£o *escolhida*)
            td_target = reward + gamma * q_table[next_state, next_action]
            td_error = td_target - q_table[state, action]
            q_table[state, action] += alpha * td_error

            state = next_state
            action = next_action
            total_reward += reward

            if done:
                break

        rewards_all_episodes.append(total_reward)

    return q_table, rewards_all_episodes, epsilons

"""# 6) Treinamento com varia√ß√£o de hiperpar√¢metros e registro de resultados"""

env = make_env()

algorithms = {
    'Q-Learning': train_q_learning,
    'SARSA': train_sarsa
}

alphas = [0.1, 0.5]
gammas = [0.9, 0.99]

results = []

num_episodes = 5000
max_steps = 100
initial_epsilon = 1.0
min_epsilon = 0.05
decay_rate = 0.999

for alg_name, alg_func in algorithms.items():
    for alpha in alphas:
        for gamma in gammas:

            q_table, rewards, eps_history = alg_func(
                env,
                num_episodes=num_episodes,
                max_steps=max_steps,
                alpha=alpha,
                gamma=gamma,
                initial_epsilon=initial_epsilon,
                min_epsilon=min_epsilon,
                decay_rate=decay_rate
            )

            rewards = np.array(rewards)
            mean_last_100 = rewards[-100:].mean()
            success_rate = np.mean(rewards)
            final_epsilon = eps_history[-1]

            results.append({
                'algorithm': alg_name,
                'alpha': alpha,
                'gamma': gamma,
                'mean_reward_last_100': mean_last_100,
                'success_rate': success_rate,
                'final_epsilon': final_epsilon,
                'rewards_series': rewards
            })

summary_df = pd.DataFrame([
    {k: v for k, v in r.items() if k != 'rewards_series'}
    for r in results
])

print("\n--- Resumo dos Resultados ---")
print(summary_df)

for alg_name in algorithms.keys():
    subset = summary_df[summary_df['algorithm'] == alg_name]
    pivot = subset.pivot(index='alpha', columns='gamma', values='mean_reward_last_100')
    plt.figure(figsize=(6, 4))
    sns.heatmap(pivot, annot=True, fmt='.3f', cmap='viridis', cbar_kws={'label': 'M√©dia de Recompensa (√öltimos 100)'})
    plt.title(f'M√©dia de Recompensa (alg={alg_name})')
    plt.show()

"""# Salvar resumo em CSV"""

summary_df.to_csv('rl_summary_frozenlake.csv', index=False)
print('\nResumo salvo em rl_summary_frozenlake.csv')